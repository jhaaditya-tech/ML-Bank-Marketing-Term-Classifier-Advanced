{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading and Extracting Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Extracting dataset...\n",
      "Dataset loaded successfully.\n",
      "First few rows of the dataset:\n",
      "   age        job  marital    education  default housing loan    contact  \\\n",
      "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
      "1   57   services  married  high.school  unknown      no   no  telephone   \n",
      "2   37   services  married  high.school       no     yes   no  telephone   \n",
      "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
      "4   56   services  married  high.school       no      no  yes  telephone   \n",
      "\n",
      "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
      "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
      "\n",
      "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
      "0          93.994          -36.4      4.857       5191.0  no  \n",
      "1          93.994          -36.4      4.857       5191.0  no  \n",
      "2          93.994          -36.4      4.857       5191.0  no  \n",
      "3          93.994          -36.4      4.857       5191.0  no  \n",
      "4          93.994          -36.4      4.857       5191.0  no  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Download and Extract Dataset\n",
    "dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip'\n",
    "dataset_path = 'bank-additional.zip'\n",
    "extracted_folder = 'bank-additional'\n",
    "\n",
    "# Download and Extract Dataset\n",
    "print(\"Downloading dataset...\")\n",
    "urllib.request.urlretrieve(dataset_url, dataset_path)\n",
    "print(\"Extracting dataset...\")\n",
    "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)\n",
    "\n",
    "# Load the CSV file\n",
    "data_file_path = os.path.join(extracted_folder, 'bank-additional', 'bank-additional-full.csv')\n",
    "data = pd.read_csv(data_file_path, sep=';')\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target column to binary\n",
    "data['y'] = data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Splitting the Data into Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 4: Handling Class Balance using SMOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feature Engineering - Dimensonality Redcution using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "X_resampled_pca = pca.fit_transform(X_resampled)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Hyperparameter Tuning for Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  20.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  20.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  20.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  20.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  20.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  21.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  21.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  21.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  21.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  15.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  15.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  15.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  24.4s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  24.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  25.2s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  25.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  43.8s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  44.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  41.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  42.9s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  42.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  43.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  45.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  19.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  19.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  19.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  15.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  15.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  41.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  15.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  42.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  40.7s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  41.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  42.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  43.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  45.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  15.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  44.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  15.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  16.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  47.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  48.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  49.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  38.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  37.4s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  36.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  18.7s\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],  # Reduced the number of trees to make tuning faster\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=param_grid_rf, \n",
    "    n_iter=20,  \n",
    "    cv=3,  \n",
    "    n_jobs=-1, \n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "random_search_rf.fit(X_resampled_pca, y_resampled)\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 - SVM Model Training with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 1.1min\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 1.1min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 1.8min\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 1.8min\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 1.8min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 1.9min\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time= 2.8min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time= 3.6min\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time= 3.6min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 1.8min\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time= 3.4min\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time= 3.5min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 2.0min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 2.5min\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time= 2.4min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time= 1.6min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 3.6min\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time= 3.9min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time= 1.3min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time= 2.4min\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time= 2.6min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 3.8min\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time= 3.8min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time= 1.6min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time= 3.9min\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time= 3.7min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time= 3.3min\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time= 3.0min\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time= 9.1min\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time= 9.3min\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=15.9min\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=14.2min\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=16.0min\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=14.3min\n"
     ]
    }
   ],
   "source": [
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    estimator=svm, \n",
    "    param_distributions=param_grid_svm, \n",
    "    n_iter=12, \n",
    "    cv=3, \n",
    "    n_jobs=-1, \n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "random_search_svm.fit(X_resampled_pca, y_resampled)\n",
    "best_svm = random_search_svm.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7 - Cross Validation using Stratified K-Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross-Validation Scores: [0.93863592 0.93305971 0.93775042 0.94126845 0.93540506]\n",
      "Random Forest Mean Cross-Validation Score: 0.9372239101745434\n",
      "SVM Cross-Validation Scores: [0.94420559 0.93960715 0.94068211 0.94732727 0.94312518]\n",
      "SVM Mean Cross-Validation Score: 0.9429894614367397\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "cross_val_rf = cross_val_score(best_rf, X_resampled_pca, y_resampled, cv=skf)\n",
    "cross_val_svm = cross_val_score(best_svm, X_resampled_pca, y_resampled, cv=skf)\n",
    "\n",
    "print(\"Random Forest Cross-Validation Scores:\", cross_val_rf)\n",
    "print(\"Random Forest Mean Cross-Validation Score:\", cross_val_rf.mean())\n",
    "print(\"SVM Cross-Validation Scores:\", cross_val_svm)\n",
    "print(\"SVM Mean Cross-Validation Score:\", cross_val_svm.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation Test Set Using Random Forest Evlaution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Set Evaluation:\n",
      "Accuracy: 0.8945536942623614\n",
      "Precision: 0.5285073670723895\n",
      "Recall: 0.5926724137931034\n",
      "F1 Score: 0.558753809685066\n",
      "Confusion Matrix:\n",
      " [[10229   736]\n",
      " [  567   825]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = best_rf.predict(X_test_pca)\n",
    "y_pred_rf_proba = best_rf.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Test Set Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation Test Set Using SVM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Test Set Evaluation:\n",
      "Accuracy: 0.9058833050093065\n",
      "Precision: 0.5809187279151944\n",
      "Recall: 0.5905172413793104\n",
      "F1 Score: 0.5856786604916281\n",
      "Confusion Matrix:\n",
      " [[10372   593]\n",
      " [  570   822]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_svm = best_svm.predict(X_test_pca)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "print(\"SVM Test Set Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "print(\"Precision:\", precision_svm)\n",
    "print(\"Recall:\", recall_svm)\n",
    "print(\"F1 Score:\", f1_svm)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting Decision Threshold for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.1, Precision: 0.24645683784281244, Recall: 0.9619252873563219, F1 Score: 0.3923809523809524\n",
      "Threshold: 0.2, Precision: 0.34455172413793106, Recall: 0.8972701149425287, F1 Score: 0.49790711580625874\n",
      "Threshold: 0.30000000000000004, Precision: 0.42626034612490593, Recall: 0.8139367816091954, F1 Score: 0.5595061728395062\n",
      "Threshold: 0.4, Precision: 0.4717607973421927, Recall: 0.7140804597701149, F1 Score: 0.5681623320948842\n",
      "Threshold: 0.5, Precision: 0.526117054751416, Recall: 0.6005747126436781, F1 Score: 0.5608856088560885\n",
      "Threshold: 0.6000000000000001, Precision: 0.5799043062200957, Recall: 0.4353448275862069, F1 Score: 0.49733278621255644\n",
      "Threshold: 0.7000000000000001, Precision: 0.6412478336221837, Recall: 0.26580459770114945, F1 Score: 0.37582529202640935\n",
      "Threshold: 0.8, Precision: 0.7030567685589519, Recall: 0.11566091954022989, F1 Score: 0.1986428130783467\n"
     ]
    }
   ],
   "source": [
    "# y_pred_new_rf = (y_pred_rf_proba >= 0.3).astype(int)\n",
    "# precision_rf_new = precision_score(y_test, y_pred_new_rf)\n",
    "# recall_rf_new = recall_score(y_test, y_pred_new_rf)\n",
    "# f1_rf_new = f1_score(y_test, y_pred_new_rf)\n",
    "\n",
    "# print(\"Random Forest with Adjusted Threshold (0.3):\")\n",
    "# print(\"Precision:\", precision_rf_new)\n",
    "# print(\"Recall:\", recall_rf_new)\n",
    "# print(\"F1 Score:\", f1_rf_new)\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "for threshold in thresholds:\n",
    "    y_pred_new_rf = (y_pred_rf_proba >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y_pred_new_rf)\n",
    "    recall = recall_score(y_test, y_pred_new_rf)\n",
    "    f1 = f1_score(y_test, y_pred_new_rf)\n",
    "    print(f\"Threshold: {threshold}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final recommendation Based on Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM is recommended based on better F1 score.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Final Recommendations based on Evaluation\n",
    "if f1_rf_new > f1_svm:\n",
    "    print(\"Random Forest with adjusted threshold is recommended based on improved F1 score.\")\n",
    "elif f1_svm > f1_rf_new:\n",
    "    print(\"SVM is recommended based on better F1 score.\")\n",
    "else:\n",
    "    print(\"Both models perform similarly; further tuning might be necessary.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
